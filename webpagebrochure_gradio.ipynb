{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ebfc7ce-3387-4d74-a9dd-4a0668ccc7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "# Define headers for web scraping\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Class to fetch webpage content\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.text = \"\"\n",
    "        self.title = \"No title found\"\n",
    "        self.links = []\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)  # Set timeout to avoid hanging\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors (4xx, 5xx)\n",
    "            self.body = response.content\n",
    "            self.soup = BeautifulSoup(self.body, 'html.parser')\n",
    "            self.title = self.soup.title.string if self.soup.title else \"No title found\"\n",
    "\n",
    "            # Clean up text by removing unnecessary elements\n",
    "            if self.soup.body:\n",
    "                for irrelevant in self.soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                    irrelevant.decompose()\n",
    "                self.text = self.soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # Extract links\n",
    "            links = [link.get('href') for link in self.soup.find_all('a')]\n",
    "            self.links = [urljoin(self.url, link) for link in links if link]\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Catch all request-related errors (e.g., invalid URL, timeout, network issues)\n",
    "            self.text = \"\"  # Empty text indicates a failure\n",
    "            self.links = []\n",
    "\n",
    "# Function to filter relevant links using Ollama\n",
    "def filter_relevant_links(links):\n",
    "    if not links:\n",
    "        return []\n",
    "\n",
    "    links_text = \"\\n\".join(links)\n",
    "    prompt = f\"\"\"\n",
    "    Here is a list of URLs extracted from a company's website:\n",
    "\n",
    "    {links_text}\n",
    "\n",
    "    Your task:\n",
    "    - Identify URLs that would be most relevant to include in a brochure about the company, such as About page, Careers/Job page, Services.\n",
    "    - Ignore links to terms of service, privacy, login pages, or external sites.\n",
    "    - Return only relevant URLs without any additional text or explanations.\n",
    "\n",
    "    Respond with the filtered list.\n",
    "    \"\"\"\n",
    "\n",
    "    response = None\n",
    "    while response is None:  # Keep retrying until we get a valid response\n",
    "        try:\n",
    "            response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(15)  # Wait for 15 seconds before retrying\n",
    "\n",
    "    relevant_links = response['message']['content'].split(\"\\n\")\n",
    "    return [link.strip() for link in relevant_links if link.strip() and link.startswith(\"http\")]\n",
    "\n",
    "# Function to generate the brochure\n",
    "def generate_brochure_from_contents(scraped_text):\n",
    "    prompt = f\"\"\"\n",
    "    Create a brochure based on the following website content:,\n",
    "    {scraped_text}\n",
    "    Your task is to generate a professional brochure without any picture, page number, cover, etc. that highlights key information about the company, such as:\n",
    "    - The title should be company name only, nothing else.\n",
    "    - Services\n",
    "    - Products\n",
    "    - Values & Mission\n",
    "    - Webpage\n",
    "    - Career (only if available, else discard this part from being included in the brochure)\n",
    "\n",
    "    The brochure should be detailed and informative. Do not include your talk other than brochure input. It should be focused solely on the content provided in the prompt without introducing any irrelevant material or placeholders like '[Cover Page]', '[Insert Twitter Handle]', or similar.\n",
    "    Do not mention where to insert pictures. Ensure all content is strictly from the provided context, and avoid referencing external sites or inserting placeholder links. Do not hallucinate text.\n",
    "    The brochure should only contain the relevant information without any extraneous formatting or mentions of page numbers like 'Cover page', 'Company Logo', 'Page 1', 'Back Cover', etc.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant tasked with generating a professional brochure from a company's web content. Follow these rules:\n",
    "    1. Do not add unnecessary formatting (e.g., page numbers, cover page, or picture placeholders).\n",
    "    2. Focus only on the company's relevant information.\n",
    "    3. Discard any irrelevant content, especially external links, and avoid introducing content like 'Cover Page', 'Back Cover', or placeholders.\n",
    "    4. Organize the information in a clear, concise manner and do not insert unnecessary page numbers, external links, or picture placeholders.\n",
    "    5. No assumptions or hallucinations. If some sections are missing from the content, exclude them entirely.\n",
    "    6. No disclaimers or commentary. Do not add phrases like \"Here is your brochure\" or \"This content follows the rules.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = None\n",
    "    while response is None:  # Keep retrying until we get a valid response\n",
    "        try:\n",
    "            response = ollama.chat(model=\"llama3.2\", messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "# Convert to markdown format\n",
    "def convert_to_markdown(content):\n",
    "    markdown = \"\"\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith(\"**\"):\n",
    "            markdown += f\"## {line.strip('**')}\\n\"\n",
    "        elif line.startswith(\"*\"):\n",
    "            markdown += f\"- {line[2:]}\\n\"\n",
    "        else:\n",
    "            markdown += f\"{line}\\n\"\n",
    "    \n",
    "    return markdown\n",
    "\n",
    "# Function to process the URL and generate a brochure\n",
    "def process_url(url):\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url  # Ensure URL starts with \"http\" or \"https\"\n",
    "\n",
    "    # Show the initial message\n",
    "    yield \"<p style='text-align: center; font-size: 18px;'><br><b><b>üîÑ Please wait, your brochure is being generated... üîÑ</p>\"\n",
    "\n",
    "    # Fetch the website\n",
    "    web = Website(url)\n",
    "\n",
    "    # Reset the variables to avoid stale data\n",
    "    filter_links = []  \n",
    "    web_contents = []  \n",
    "\n",
    "    # Check if the website returned valid content\n",
    "    if not web.text.strip():\n",
    "        yield \"<p style='text-align: center; font-size: 18px;'><br><b><b>‚ùå Unable to fetch content. Please check the URL and try again. ‚ùå</p>\"\n",
    "        return\n",
    "\n",
    "    # Step 1: Get filtered links\n",
    "    filter_links = filter_relevant_links(web.links)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    if not filter_links:  # If no relevant links are found, return an error message\n",
    "        yield \"<p style='text-align: center; font-size: 18px;'><br><b><b>‚ùå No relevant links found. Try again. ‚ùå</p>\"\n",
    "        return\n",
    "\n",
    "    # Step 2: Extract text from each relevant link\n",
    "    #web_contents = []\n",
    "    for link in filter_links:\n",
    "        web_page = Website(link)  \n",
    "        \n",
    "        # Filter out lines with at most 3 words\n",
    "        filtered_text = \"\\n\".join(line for line in web_page.text.split(\"\\n\") if len(line.split()) > 3)\n",
    "\n",
    "        if filtered_text.strip():\n",
    "            web_contents.append((web_page.title, link, filtered_text))\n",
    "\n",
    "    if not web_contents:  # No useful content extracted\n",
    "        yield \"<p style='text-align: center; font-size: 18px;'><br><b><b>‚ùå Extracted content is too short or irrelevant. Try again. ‚ùå</p>\"\n",
    "        return\n",
    "\n",
    "    # Step 3: Prepare text for Ollama\n",
    "    scraped_text = \"\\n\\n\".join([f\"Title: {title}\\nLink: {link}\\nContent: {text}\" for title, link, text in web_contents])\n",
    "\n",
    "    # Step 4: Generate brochure\n",
    "    brochure = generate_brochure_from_contents(scraped_text)\n",
    "\n",
    "    # Convert to Markdown\n",
    "    markdown_brochure = convert_to_markdown(brochure)\n",
    "\n",
    "    # Final output replaces \"Please wait\" message\n",
    "    yield markdown_brochure\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks(theme='monochrome') as iface:\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center;'>Company Brochure Generator</h1>\"\n",
    "        \"<p style='text-align: center;'>Enter a company website URL to generate a markdown-formatted brochure.</p>\"\n",
    "    )\n",
    "\n",
    "    inp = gr.Textbox(label=\"Web-page URL\", placeholder=\"Enter Website URL\")\n",
    "    btn = gr.Button(\"Generate Brochure\")  \n",
    "    out = gr.Markdown(label=\"Generated Brochure\")\n",
    "\n",
    "    btn.click(process_url, inputs=inp, outputs=out)\n",
    "\n",
    "# Launch Gradio App\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9ca04-ea79-4790-bf37-074c76bbd474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
