{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3be1db-6546-4a2e-95c4-016e237e10a6",
   "metadata": {},
   "source": [
    "# LLM Project: Company Brochure generator using Llama 3.2 with help of web-scraping\n",
    "\n",
    "This program fetches content of a company webpage via user-input and then will with help of web-scraping it and LLM (Llama 3.2). \n",
    "\n",
    "It will go through relevant sub-links to fetch contents and generate a brochure for the company.\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "To begin, we need the following Python libraries:\n",
    "- `requests`: To fetch the webpage content.\n",
    "- `beautifulsoup4`: To parse and clean up the webpage HTML.\n",
    "- `ollama`: To interface with the locally installed Llama 3.2 model.\n",
    "\n",
    "Once the libraries has been installed in your environment, open up a Jupyter notebook and proceed to next steps.\n",
    "\n",
    "### Step 2: Fetch Webpage Content\n",
    "A class 'Website' is created. This class:\n",
    "- Takes a URL as input.\n",
    "- Sends a request to fetch the webpage.\n",
    "- Uses a user-agent header to mimic a real browser request.\n",
    "- Returns the HTML content and gets it parsed using BeautifulSoup.\n",
    "- Removes redundant contents from the parsed HTML.\n",
    "- Extracts sub-links within the main URL page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3d3d4d57-7b6f-40d7-95bf-940e99637d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to fetch main webpage \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Defining a dictionary \"headers\" to mimic a real web browser request.\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Creating a class that will store webpage content, title and links.\n",
    "\n",
    "class Website:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)                                 # Makes an HTTP GET request to the given URL with pre-defined headers\n",
    "        self.body = response.content                                                  # Stores the raw HTML of the page\n",
    "        self.soup = BeautifulSoup(self.body, 'html.parser')                           # Parses the HTML content using html.parser\n",
    "        self.title = self.soup.title.string if self.soup.title else \"No title found\"  # Extracts title of webpage\n",
    "        \n",
    "        if self.soup.body:                                                            # Cleaning unneccesary elements such as <script>, <style>, <img>, <input>\n",
    "            for irrelevant in self.soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = self.soup.body.get_text(separator=\"\\n\", strip=True)           # seperator=\"\\n\" seperates each text block with a new line. strip=True gets rid off unwanted spaces.\n",
    "            \n",
    "        else:\n",
    "            self.text = \"\"\n",
    "            \n",
    "        \"\"\"\n",
    "        ***EXTRACTING LINKS*** \n",
    "        \n",
    "        Now we need to extract all links available in the webpage.\n",
    "        soup.find_all('a') finds all anchor <a> elements in the parsed HTML which contains hyperlinks.\n",
    "\n",
    "        For example:\n",
    "        \n",
    "        If HTML content is:\n",
    "        \n",
    "        <html>\n",
    "            <body>\n",
    "                <a href=\"https://example.com\">Example</a>\n",
    "                <a href=\"https://google.com\">Google</a>\n",
    "                <a>Broken Link</a>  <!-- No href attribute -->\n",
    "            </body>\n",
    "        </html>\n",
    "\n",
    "        Upon parsing using soup.find.all('a') we will get output:\n",
    "\n",
    "        [\n",
    "            <a href=\"https://example.com\">Example</a>,\n",
    "            <a href=\"https://google.com\">Google</a>,\n",
    "            <a>Broken Link</a>\n",
    "        ]\n",
    "\n",
    "        To extract only href attribute links we use link.get('href'). <a> tags with no href would return \"None\".\n",
    "        i.e. \n",
    "\n",
    "        [\n",
    "            \"https://example.com\",\n",
    "            \"https://google.com\",\n",
    "            None\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        links = [link.get('href') for link in self.soup.find_all('a')]\n",
    "        \n",
    "        \"\"\"Now we iterate over every \"link\" in \"links\" to filter out the None values and keeping the dictionary \"links\" with only valid URLs\"\"\"\n",
    "        self.links = [link for link in links if link]   \n",
    "\n",
    "        # Convert relative URLs to absolute URLs\n",
    "        self.links = [urljoin(self.url, link) for link in self.links]\n",
    "\n",
    "    def get_contents(self):                                                        \n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\" # Defining a method to print return webpage title and extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21696ae5-a54c-4f18-80ab-2077f042d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a website URL:  https://huggingface.co/\n"
     ]
    }
   ],
   "source": [
    "url = input(\"Enter a website URL: \")  # Prompt the user for a URL\n",
    "if not url.startswith(\"http\"):        # Ensure it starts with http or https\n",
    "    url = \"https://\" + url\n",
    "\n",
    "web = Website(url)                    # Create a Website object with the user-provided URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3669a0-e4c1-4ad0-8ab2-6eb3e0df5a2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing and analysis\n",
    "Following cells are to test output of 'Website' class using user-input link ('web').\n",
    "**You can proceed to Step 3 if no testing is required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2ed18fe7-3353-47c6-8c33-e0b6bf978164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face – The AI community building the future.'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title of webpage\n",
    "web.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2aea7-3c1c-4ab1-9932-ec51ffc153e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Raw HTML content\n",
    "print(web.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01679f3d-f024-4d60-893e-2581ba56c5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parsed HTML content\n",
    "print(web.soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737fd9b-19b4-4cae-8bc9-5530822ac85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaned parsed text\n",
    "print(web.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d0b86a18-132e-489a-8c34-50c5b8f2317b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://huggingface.co/',\n",
       " 'https://huggingface.co/models',\n",
       " 'https://huggingface.co/datasets',\n",
       " 'https://huggingface.co/spaces',\n",
       " 'https://huggingface.co/posts',\n",
       " 'https://huggingface.co/docs',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/pricing',\n",
       " 'https://huggingface.co/login',\n",
       " 'https://huggingface.co/join',\n",
       " 'https://huggingface.co/spaces',\n",
       " 'https://huggingface.co/models',\n",
       " 'https://huggingface.co/Wan-AI/Wan2.1-T2V-14B',\n",
       " 'https://huggingface.co/microsoft/Phi-4-multimodal-instruct',\n",
       " 'https://huggingface.co/deepseek-ai/DeepSeek-R1',\n",
       " 'https://huggingface.co/perplexity-ai/r1-1776',\n",
       " 'https://huggingface.co/allenai/olmOCR-7B-0225-preview',\n",
       " 'https://huggingface.co/models',\n",
       " 'https://huggingface.co/spaces/Wan-AI/Wan2.1',\n",
       " 'https://huggingface.co/spaces/nanotron/ultrascale-playbook',\n",
       " 'https://huggingface.co/spaces/huggingface/ai-deadlines',\n",
       " 'https://huggingface.co/spaces/black-forest-labs/FLUX.1-dev',\n",
       " 'https://huggingface.co/spaces/lllyasviel/LuminaBrush',\n",
       " 'https://huggingface.co/spaces',\n",
       " 'https://huggingface.co/datasets/facebook/natural_reasoning',\n",
       " 'https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k',\n",
       " 'https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified',\n",
       " 'https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT',\n",
       " 'https://huggingface.co/datasets/allenai/olmOCR-mix-0225',\n",
       " 'https://huggingface.co/datasets',\n",
       " 'https://huggingface.co/join',\n",
       " 'https://huggingface.co/pricing#endpoints',\n",
       " 'https://huggingface.co/pricing#spaces',\n",
       " 'https://huggingface.co/pricing',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/enterprise',\n",
       " 'https://huggingface.co/allenai',\n",
       " 'https://huggingface.co/facebook',\n",
       " 'https://huggingface.co/amazon',\n",
       " 'https://huggingface.co/google',\n",
       " 'https://huggingface.co/Intel',\n",
       " 'https://huggingface.co/microsoft',\n",
       " 'https://huggingface.co/grammarly',\n",
       " 'https://huggingface.co/Writer',\n",
       " 'https://huggingface.co/docs/transformers',\n",
       " 'https://huggingface.co/docs/diffusers',\n",
       " 'https://huggingface.co/docs/safetensors',\n",
       " 'https://huggingface.co/docs/huggingface_hub',\n",
       " 'https://huggingface.co/docs/tokenizers',\n",
       " 'https://huggingface.co/docs/trl',\n",
       " 'https://huggingface.co/docs/transformers.js',\n",
       " 'https://huggingface.co/docs/smolagents',\n",
       " 'https://huggingface.co/docs/peft',\n",
       " 'https://huggingface.co/docs/datasets',\n",
       " 'https://huggingface.co/docs/text-generation-inference',\n",
       " 'https://huggingface.co/docs/accelerate',\n",
       " 'https://huggingface.co/models',\n",
       " 'https://huggingface.co/datasets',\n",
       " 'https://huggingface.co/spaces',\n",
       " 'https://huggingface.co/tasks',\n",
       " 'https://ui.endpoints.huggingface.co',\n",
       " 'https://huggingface.co/chat',\n",
       " 'https://huggingface.co/huggingface',\n",
       " 'https://huggingface.co/brand',\n",
       " 'https://huggingface.co/terms-of-service',\n",
       " 'https://huggingface.co/privacy',\n",
       " 'https://apply.workable.com/huggingface/',\n",
       " 'mailto:press@huggingface.co',\n",
       " 'https://huggingface.co/learn',\n",
       " 'https://huggingface.co/docs',\n",
       " 'https://huggingface.co/blog',\n",
       " 'https://discuss.huggingface.co',\n",
       " 'https://status.huggingface.co/',\n",
       " 'https://github.com/huggingface',\n",
       " 'https://twitter.com/huggingface',\n",
       " 'https://www.linkedin.com/company/huggingface/',\n",
       " 'https://huggingface.co/join/discord',\n",
       " 'https://www.zhihu.com/org/huggingface',\n",
       " 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/chinese-language-blog/wechat.jpg']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of links\n",
    "web.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6bf4d-e8f3-44b0-8636-759386742696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing get_contents() method\n",
    "print(web.get_contents())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be789434-44e5-4386-9cc2-47e4545e4091",
   "metadata": {},
   "source": [
    "### Step 3: Using LLM to decide the relevant links\n",
    "\n",
    "We need to extract only relevant website URLs from the scraped contents instead of going through every link for further scraping. \n",
    "Therefore we use the help of LLM to decide which links are relevant.\n",
    "\n",
    "- Create a LLM function called `filter_relevant_links` in which the LLM model is assigned to decide which links are relevant\n",
    "- Using this created function, we fetch the relevant links in `web.links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f3bf8f43-3605-4c73-af42-5c78a2f8ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to filter relevant links using Ollama\n",
    "def filter_relevant_links(links, prompt=\"Find relevant URLs related to company services and products.\"):\n",
    "    \"\"\"\n",
    "    Uses Ollama to analyze extracted URLs and filter relevant ones based on a prompt.\n",
    "    \"\"\"\n",
    "    # Format links into a text block for LLM processing\n",
    "    links_text = \"\\n\".join(links)\n",
    "    \n",
    "    # Create a structured prompt for Ollama\n",
    "    full_prompt = f\"\"\"\n",
    "    Here is a list of URLs extracted from a company's website:\n",
    "\n",
    "    {links_text}\n",
    "\n",
    "    Your task:\n",
    "    - Identify URLs that would be most relevant to include in a brochure about the company, such as About page, Careers/Job page, Services.\n",
    "    - Ignore links to terms of service, privacy, login pages, or external sites.\n",
    "    - Return only relevant URLs without any additional text or explanations.\n",
    "\n",
    "    Respond with the filtered list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Query Ollama\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": full_prompt}])\n",
    "    \n",
    "    # Extract response and return\n",
    "    relevant_links = response['message']['content'].split(\"\\n\")\n",
    "\n",
    "    # Filter out empty lines and unwanted text\n",
    "    return [link.strip() for link in relevant_links if link.strip() and link.startswith(\"http\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fa707e70-0298-4051-976d-7757f7453e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: If list below is empty, try again after few seconds since the LLM is still generating output.\n",
      "\n",
      "['https://huggingface.co/about', 'https://huggingface.co/join', 'https://huggingface.co/pricing#services', 'https://huggingface.co/learn', 'https://huggingface.co/blog', 'https://discuss.huggingface.co', 'https://status.huggingface.co', 'https://github.com/huggingface', 'https://twitter.com/huggingface', 'https://www.linkedin.com/company/huggingface/', 'https://huggingface.co/join/discord']\n"
     ]
    }
   ],
   "source": [
    "filter_links = filter_relevant_links(web.links)\n",
    "print(\"WARNING: If list below is empty, try again after few seconds since the LLM is still generating output.\\n\")\n",
    "print(filter_links) # Prints out the relevant links decided by LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203713d-6c00-4972-a851-79a81b9958eb",
   "metadata": {},
   "source": [
    "## Step 4: Accessing every relevant link and scraping contents\n",
    "\n",
    "Every link from the LLM decision (`filter_links`) is accessed for scrapping using the `Website()` function created in Step 2.\n",
    "\n",
    "The title, link and content of each of these websites is listed and stored in `web_contents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2d9e49b0-f2b5-4172-a3ba-b2ee3bde08e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: about (Sergei)\n",
      "Link: https://huggingface.co/about\n",
      "Content: AI & ML interests...\n",
      "\n",
      "Title: No title found\n",
      "Link: https://huggingface.co/join\n",
      "Content: ...\n",
      "\n",
      "Title: Hugging Face – Pricing\n",
      "Link: https://huggingface.co/pricing#services\n",
      "Content: Leveling up AI collaboration and compute.\n",
      "Users and organizations already use the Hub as a collaboration platform,\n",
      "we’re making it easy to seamlessly and scalably launch ML compute directly from the Hub.\n",
      "Collaborate on Machine Learning\n",
      "Host unlimited public models, datasets\n",
      "Create unlimited orgs with no member limits\n",
      "Access the latest ML tools and open source\n",
      "Unlock advanced HF features\n",
      "ZeroGPU and Dev Mode for Spaces\n",
      "Higher rate limits for serverless inference\n",
      "Get early access to upcoming featu...\n",
      "\n",
      "Title: Hugging Face - Learn\n",
      "Link: https://huggingface.co/learn\n",
      "Content: This course will teach you about natural language processing using libraries from the HF ecosystem\n",
      "Learn to build and deploy your own AI agents\n",
      "This course will teach you about deep reinforcement learning using libraries from the HF ecosystem\n",
      "Community Computer Vision Course\n",
      "This course will teach you about computer vision ML using libraries and models from the HF ecosystem\n",
      "Learn to apply transformers to audio data using libraries from the HF ecosystem\n",
      "A collection of open-source-powered noteboo...\n",
      "\n",
      "Title: Hugging Face – Blog\n",
      "Link: https://huggingface.co/blog\n",
      "Content: Blog, Articles, and discussions\n",
      "Trace & Evaluate your Agent with Arize Phoenix\n",
      "Making Browser-Based Inference Actually Usable\n",
      "February 2025 Digest - Cognitive Financial Agents\n",
      "The Theory of Informational Relative Evolution\n",
      "Deep Learning GPU Benchmarks\n",
      "LettuceDetect: A Hallucination Detection Framework for RAG Applications\n",
      "Scaling Expert judgment with Large Language Models (LLM-as-a-Judge)\n",
      "❤️ a love letter to the Open AI inference client\n",
      "DualPipe Explained: A Comprehensive Guide to DualPipe That ...\n",
      "\n",
      "Title: Hugging Face Forums - Hugging Face Community Discussion\n",
      "Link: https://discuss.huggingface.co\n",
      "Content: Need guidance on Adding ASR Support to Auto Train UI\n",
      "TypeError: stat: path trying to start model\n",
      "Public archive of data for preservation\n",
      "Can't access HF discord after verification, been trying over a month\n",
      "Using PDFs in the chat hardly works\n",
      "Model with Genrate method to torchscript\n",
      "Seeking Hugging Face Users for Casual Chat About AI Model Openness\n",
      "Authentication issue while cloning repo locally\n",
      "Looking for \"How-to\" on training with multiple files\n",
      "Can't Access Profile or Spaces this Morning\n",
      "Llama...\n",
      "\n",
      "Title: \n",
      "Hugging Face status\n",
      "\n",
      "Link: https://status.huggingface.co\n",
      "Content: Get e-mail notifications whenever Hugging Face creates, updates or resolves an incident.\n",
      "Subscribe to specific components\n",
      "Current status by service\n",
      "Git Hosting and Serving\n",
      "All services are online\n",
      "Last updated on Mar 02 at 02:29pm EST\n",
      "Current status by service\n",
      "Git Hosting and Serving...\n",
      "\n",
      "Title: Hugging Face · GitHub\n",
      "Link: https://github.com/huggingface\n",
      "Content: Write better code with AI\n",
      "Find and fix vulnerabilities\n",
      "Plan and track work\n",
      "Collaborate outside of code\n",
      "Find more, search less\n",
      "Small and medium teams\n",
      "View all use cases\n",
      "Fund open source developers\n",
      "Search or jump to...\n",
      "Search code, repositories, users, issues, pull requests...\n",
      "We read every piece of feedback, and take your input very seriously.\n",
      "Include my email address so I can be contacted\n",
      "Use saved searches to filter your results more quickly\n",
      "You signed in with another tab or window.\n",
      "to refresh ...\n",
      "\n",
      "Title: No title found\n",
      "Link: https://twitter.com/huggingface\n",
      "Content: JavaScript is not available.\n",
      "We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.\n",
      "© 2025 X Corp.\n",
      "Something went wrong, but don’t fret — let’s give it another shot.\n",
      "Some privacy related extensions may cause issues on x.com. Please disable them and try again....\n",
      "\n",
      "Title: Hugging Face | LinkedIn\n",
      "Link: https://www.linkedin.com/company/huggingface/\n",
      "Content: LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including\n",
      "professional and job ads\n",
      ") on and off LinkedIn. Learn more in our\n",
      "Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your\n",
      "Skip to main content\n",
      "The AI community building the future.\n",
      "Discover all 502 employees\n",
      "The AI community building the future.\n",
      "External link for Hug...\n",
      "\n",
      "Title: Hugging Face\n",
      "Link: https://huggingface.co/join/discord\n",
      "Content: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store extracted titles and cleaned texts\n",
    "web_contents = []\n",
    "\n",
    "# Loop through each filtered link and extract its title and text\n",
    "for link in filter_links:\n",
    "    web_page = Website(link)  # Create a Website object\n",
    "    \n",
    "    # Filter out lines with at most 3 words\n",
    "    filtered_text = \"\\n\".join(line for line in web_page.text.split(\"\\n\") if len(line.split()) > 3)\n",
    "    \n",
    "    # Store title and filtered text\n",
    "    web_contents.append((web_page.title, link, filtered_text))\n",
    "\n",
    "# Print the extracted titles, links, and cleaned texts\n",
    "for title, link, text in web_contents:\n",
    "    print(f\"Title: {title}\\nLink: {link}\\nContent: {text[:500]}...\\n\")  # Print first 500 characters as preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f519bb-3b34-4e86-b876-17395eaba6e3",
   "metadata": {},
   "source": [
    "## Step 4b (Optional): Combining all the scraped contents together to be used in prompt\n",
    "\n",
    "LLM prompts typically require a single string of text rather than a list of individual items. To create this, the contents from `web_contents` (which is a list of titles, links and texts) are combined into `content_text`. This forms a singular string that includes all the relevant information to be used in the model prompt.\n",
    "\n",
    "**NOTE:** However, from testing later it is found that the LLM works better with the listed prompt input. So you can **skip this part and proceed to Step 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccb162-e407-4e65-9308-c8f30863717f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare content_text from web_contents\n",
    "content_text = \"\\n\\n\".join([f\"Title: {title}\\nLink: {link}\\nContent: {text}\" for title, link, text in web_contents])\n",
    "\n",
    "# Print content_text to inspect the combined website content\n",
    "print(\"Content Text being sent to Ollama:\\n\\n\", content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a0110-0b30-4582-8fe6-967be6442c7e",
   "metadata": {},
   "source": [
    "## Step 5: Using LLM to generate the brochure\n",
    "\n",
    "Once `content_text` is ready, we can now assign another LLM prompt in order to use the information collected to produce a brochure. \n",
    "This is done by following procedure:\n",
    "- Create a LLM function `generate_brochure_from_contents` to generate the brochure where the input will be scrapped contents.\n",
    "- Pay attention to the `prompt` (a.k.a user-prompt) and `system-prompt`  as it defines how you will get your output.\n",
    "- Once the function is ready, generate the brochure using `generate_brochure_from_contents(web_contents)`\n",
    "\n",
    "\n",
    "- **NOTE #1:** `generate_brochure_from_contents(content_text)` from **Step 4b** should also work. However `generate_brochure_from_contents(web_contents)` seem to produce better output despite being a list. This might be because the structured data in the list is easier to parse through.\n",
    "  \n",
    "- **NOTE #2:** If you are not satisfied with the generated brochure, try running `brochure = generate_brochure_from_contents(web_contents)` again. You should get a good output in 2 to 3 tries for most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2589fe0c-8394-4b46-ad71-079a7a12d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a brochure using Ollama\n",
    "def generate_brochure_from_contents(scraped_text):\n",
    "    # Define the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Create a brochure based on the following website content:,\n",
    "    {scraped_text}\n",
    "    Your task is to generate a professional brochure without any picture that highlights key information about the company, such as:\n",
    "    - Services\n",
    "    - Products\n",
    "    - Values\n",
    "    - Mission\n",
    "    - Impact\n",
    "    - Webpage\n",
    "    - Contact e-mail\n",
    "    The brochure should be concise and informative. Do not include your talk other than brochure input. It should be focused solely on the content provided in the prompt without introducing any irrelevant material or placeholders like '[Cover Page]', '[Insert Twitter Handle]', or similar.\n",
    "    Do not mention where to insert pictures. Ensure all content is strictly from the provided context, and avoid referencing external sites or inserting placeholder links.\n",
    "    The brochure should only contain the relevant information without any extraneous formatting or mentions of page numbers like 'Cover page', 'Company Logo', 'Page 1', 'Back Cover', etc.\n",
    "    \"\"\"\n",
    "    # Add a system prompt to define instructions and control behavior\n",
    "    system_prompt =  f\"\"\"You are a helpful assistant tasked with generating a professional brochure from a large chunk of web-scraped text. Follow these instructions:\n",
    "    1. Focus only on the content that is directly related to the company, its services, products, values, and mission.\n",
    "    2. Discard any irrelevant content, especially external links, and avoid introducing content like 'Cover Page', 'Back Cover', or placeholders.\n",
    "    3. Organize the information in a clear, concise manner and do not insert unnecessary page numbers, external links, or picture placeholders.\n",
    "    \"\"\"\n",
    "    # Send the prompt to Ollama\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    # Extract and return the response content\n",
    "    brochure_content = response['message']['content']\n",
    "    return brochure_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0aec3a4e-a7a2-4d42-86a9-6efce7fd017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the brochure\n",
    "brochure = generate_brochure_from_contents(web_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "63d94979-cf16-48b3-be0b-b5951b113b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Welcome to Hugging Face**\n",
      "\n",
      "**Empowering AI and Machine Learning**\n",
      "\n",
      "At Hugging Face, we are dedicated to making cutting-edge machine learning and natural language processing (NLP) technologies accessible to everyone. Our mission is to democratize access to the most advanced AI models and tools, enabling researchers, developers, and organizations to build and deploy innovative applications.\n",
      "\n",
      "**Services**\n",
      "\n",
      "* **Model Hub**: We provide a vast repository of pre-trained models for various NLP tasks, making it easy for users to integrate state-of-the-art models into their applications.\n",
      "* **Inference Providers**: Our partnership with leading providers enables seamless deployment of AI models as web applications without requiring any coding expertise.\n",
      "* **Gradio**: Our open-source framework allows developers to build interactive web applications with the most advanced AI models, making it easier to showcase and share research.\n",
      "\n",
      "**Products**\n",
      "\n",
      "* **Distill-Any-Depth**: A new SOTA monocular depth estimation model trained on a large dataset, achieving state-of-the-art performance across both indoor and outdoor scenes.\n",
      "* **FastRTC**: A breakthrough technology eliminating hours of AI deployment work with a single button on Space creation from any HuggingFace model.\n",
      "\n",
      "**Values**\n",
      "\n",
      "* **Democratization**: We believe that everyone should have access to the most advanced AI models and tools, regardless of their background or expertise.\n",
      "* **Collaboration**: We foster a community of developers, researchers, and organizations working together to advance the field of NLP and machine learning.\n",
      "* **Innovation**: We are committed to pushing the boundaries of what is possible with AI and machine learning.\n",
      "\n",
      "**Mission**\n",
      "\n",
      "Our mission is to empower the world with cutting-edge AI and machine learning technologies, making it easier for everyone to build and deploy innovative applications.\n",
      "\n",
      "**Impact**\n",
      "\n",
      "By democratizing access to advanced AI models and tools, we aim to accelerate progress in various fields, including but not limited to:\n",
      "\n",
      "* Natural Language Processing\n",
      "* Computer Vision\n",
      "* Healthcare\n",
      "* Education\n",
      "\n",
      "**Webpage**\n",
      "\n",
      "Learn more about our products, services, and community at [https://huggingface.co/](https://huggingface.co/)\n",
      "\n",
      "**Contact Email**\n",
      "\n",
      "For any inquiries or collaborations, please contact us at [info@huggingface.com](mailto:info@huggingface.com)\n"
     ]
    }
   ],
   "source": [
    "print(brochure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cc3a4-b7ae-4513-8e93-14ad0e41fb11",
   "metadata": {},
   "source": [
    "## Step 6: Get your brochure in Markdown format\n",
    "\n",
    "Congratulations! You have already generated the brochure in last step. This step is just to make the text appear neater to be shown in Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "92cbbe46-cd09-4f23-b219-97b1995932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the brochure content into markdown format\n",
    "def convert_to_markdown(content):\n",
    "    # Initialize the markdown formatted content\n",
    "    markdown = \"\"\n",
    "\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        # If the line starts with a title or heading, format it as markdown header\n",
    "        if line.startswith(\"**\"):\n",
    "            markdown += f\"## {line.strip('**')}\\n\"\n",
    "        # If it starts with a bullet point, make it a markdown list\n",
    "        elif line.startswith(\"*\"):\n",
    "            markdown += f\"- {line[2:]}\\n\"\n",
    "        # Else, treat it as regular text\n",
    "        else:\n",
    "            markdown += f\"{line}\\n\"\n",
    "\n",
    "    return markdown\n",
    "\n",
    "# Convert the generated brochure content into markdown format\n",
    "markdown_brochure = convert_to_markdown(brochure)\n",
    "\n",
    "# Print the markdown formatted brochure\n",
    "#print(markdown_brochure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8b468d25-1d50-486b-9842-43e37fe63406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Welcome to Hugging Face\n",
       "\n",
       "## Empowering AI and Machine Learning\n",
       "\n",
       "At Hugging Face, we are dedicated to making cutting-edge machine learning and natural language processing (NLP) technologies accessible to everyone. Our mission is to democratize access to the most advanced AI models and tools, enabling researchers, developers, and organizations to build and deploy innovative applications.\n",
       "\n",
       "## Services\n",
       "\n",
       "- **Model Hub**: We provide a vast repository of pre-trained models for various NLP tasks, making it easy for users to integrate state-of-the-art models into their applications.\n",
       "- **Inference Providers**: Our partnership with leading providers enables seamless deployment of AI models as web applications without requiring any coding expertise.\n",
       "- **Gradio**: Our open-source framework allows developers to build interactive web applications with the most advanced AI models, making it easier to showcase and share research.\n",
       "\n",
       "## Products\n",
       "\n",
       "- **Distill-Any-Depth**: A new SOTA monocular depth estimation model trained on a large dataset, achieving state-of-the-art performance across both indoor and outdoor scenes.\n",
       "- **FastRTC**: A breakthrough technology eliminating hours of AI deployment work with a single button on Space creation from any HuggingFace model.\n",
       "\n",
       "## Values\n",
       "\n",
       "- **Democratization**: We believe that everyone should have access to the most advanced AI models and tools, regardless of their background or expertise.\n",
       "- **Collaboration**: We foster a community of developers, researchers, and organizations working together to advance the field of NLP and machine learning.\n",
       "- **Innovation**: We are committed to pushing the boundaries of what is possible with AI and machine learning.\n",
       "\n",
       "## Mission\n",
       "\n",
       "Our mission is to empower the world with cutting-edge AI and machine learning technologies, making it easier for everyone to build and deploy innovative applications.\n",
       "\n",
       "## Impact\n",
       "\n",
       "By democratizing access to advanced AI models and tools, we aim to accelerate progress in various fields, including but not limited to:\n",
       "\n",
       "- Natural Language Processing\n",
       "- Computer Vision\n",
       "- Healthcare\n",
       "- Education\n",
       "\n",
       "## Webpage\n",
       "\n",
       "Learn more about our products, services, and community at [https://huggingface.co/](https://huggingface.co/)\n",
       "\n",
       "## Contact Email\n",
       "\n",
       "For any inquiries or collaborations, please contact us at [info@huggingface.com](mailto:info@huggingface.com)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display the markdown content in the Jupyter notebook\n",
    "display(Markdown(markdown_brochure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
